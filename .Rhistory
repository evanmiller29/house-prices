library(dplyr)
# Connect to a remote database with username and password
con <- dbConnect(RMySQL::MySQL(), dbname = "caterpillar",
user = "emiller", password = "Leftie148@")
merge <- dbReadTable(con, "merge")
train_set <- dbReadTable(con, "train_set")
dbDisconnect(con)
head(merge)
head(train_set)
install.packages("rvest")
# Library declarations
library(rvest)
# Library declarations
library(rvest)
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
# Library declarations
library(rvest)
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
lego_movie %>%
html_nodes("#titleCast .itemprop span") %>%
html_text()
lego_movie %>%
html_nodes("table") %>%
.[[3]] %>%
html_table()
r.blogger <- html("http://www.r-bloggers.com/rvest-easy-web-scraping-with-r/")
r.blogger <- html("http://www.r-bloggers.com/rvest-easy-web-scraping-with-r/")
r.blogger %>%
html_nodes("#code")
r.blogger %>%
html_nodes("#code") %>%
html_text()
r.blogger %>%
html_nodes("code") %>%
html_text()
demo(package = "rvest")
help(demo)
demo(tripadvisor, package = "rvest")
stuff <- html("http://www.stuff.co.nz/")
url <- html("http://www.stuff.co.nz/")
intro <-  url %>%
html() %>%
html_nodes(".intro-text")
intro
intro <-  url %>%
html() %>%
html_nodes("#most_popular")
intro
intro <-  url %>%
html() %>%
html_nodes("#most_popular") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes("#most_popular") %>%
html_table()
intro
intro <-  url %>%
html() %>%
html_nodes("#most_popular") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes("#most_popular .section") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes(".section #most_popular ") %>%
html_text()
intro
help(gsub)
intro <-  url %>%
html() %>%
html_nodes(".section #most_popular ") %>%
html_text() %>%
gsub("\n    ", "", .)
intro
intro <-  url %>%
html() %>%
html_nodes(".section #most_popular ") %>%
html_text() %>%
gsub("\n", "", .)
intro
intro <-  url %>%
html() %>%
html_nodes(".other_headline .hot-topics #most_popular ") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes(".other_headline .hot-topics #most_popular") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes(".hbox_body div") %>%
html_text()
intro
url <- html("http://www.stackoverflow.com")
library(rvest)
url <- html("http://www.stackoverflow.com")
intro <-  url %>%
html() %>%
html_nodes("#hot-network-questions") %>%
html_text()
intro
demo(tripadisor, package = "rvest")
demo(tripadvisor, package = "rvest")
intro <-  url %>%
html() %>%
html_nodes("#hot-network-questions li a") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes("#hot-network-questions li") %>%
html_text()
intro
intro <-  url %>%
html() %>%
html_nodes("#hot-network-questions a") %>%
html_text()
intro
url <- html("http://www.stackoverflow.com")
intro <-  url %>%
html() %>%
html_nodes("#hot-network-questions a") %>%
html_text()
intro
url <- html("http://www.stuff.co.nz/")
top_stories <- url %>%
html() %>%
html_nodes("#viewed a") %>%
html_text()
top_stories
top_stories <- url %>%
html() %>%
html_nodes("#viewed a") %>%
html_text() %>%
gsub("\n                ", "", .)
top_stories
flights <- html("http://www.skyscanner.co.nz/transport/flights-from/nz/150811/150812/cheapest-flights-from-new-zealand-in-august-2015.html?adults=1&children=0&infants=0&cabinclass=economy&rtn=1&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false")
prior.mean.mean <- 0.7
prior.mean.sd <- 0.1
prior.mean.mean <- 0.7
prior.mean.sd <- 0.1
prior.sd.mean <- 0.25
prior.sd.sd <- 0.1
library(dplyr)
prior.mean.mean <- 0.7
prior.mean.sd <- 0.1
prior.sd.mean <- 0.25
prior.sd.sd <- 0.1
sigma0 <- ((prior.mean.sd / prior.mean.mean)^2 + 1) %>%
log(.) %>%
sqrt(.)
sigma0
sigma1 <- sqrt(log((prior.mean.sd / prior.mean.mean)^2 + 1)))
sigma1 <- sqrt(log((prior.mean.sd / prior.mean.mean)^2 + 1))
sigma1
mu0 <- log(prior.mean.mean) - sigma0^2 / 2
r <- c(0.958, 0.614, 0.977, 0.921, 0.756)
install.packages("MCMCpack")
library(MCMCpack)
library(MCMCpack)
library(MCMCpack)
library(dplyr)
prior.mean.mean <- 0.7
prior.mean.sd <- 0.1
prior.sd.mean <- 0.25
prior.sd.sd <- 0.1
sigma0 <- ((prior.mean.sd / prior.mean.mean)^2 + 1) %>%
log(.) %>%
sqrt(.)
mu0 <- log(prior.mean.mean) - sigma0^2 / 2
k0 <- 2 + (prior.mean.sd / prior.sd.sd) ^2
theta0 <- (k0 - - 1) * prior.sd.mean
r <- c(0.958, 0.614, 0.977, 0.921, 0.756)
r.log <- log(r)
n <- length(r)
RunSim <- function(M, delta, mu, sigma){
output.df <- data.frame(mu = rep(NA, M), sigma = NA)
set.seed(0)
cur.prior.log <- (dnorm(mu, mu0, sigma0, log = TRUE)
+ log(dinvgamma(sigma, k0, theta0)))
cur.like.log <- sum(dnorm(r.log, mu, sigma, log = TRUE))
for (i in seq_len(M)){
mu.cand <- mu + rnorm(1, sd = delta)
sigma.cand <- max(1e-5, sigma + rnorm(1, sd = delta / 2))
cand.prior.log <- (dnorm( mu.cand, mu0, sigma0, log = TRUE)
+ log(dinvgamma(sigma.cand, k0, theta0)))
cand.like.log <- sum( dnorm(r.log, mu.cand, sigma.cand, log = TRUE))
cand.ratio <- exp(cand.prior.log + cand.like.log
- cur.prior.log - cur.like.log)
if (runif(1) < cand.ratio){
mu <- mu.cand
sigma <- sigma.cand
cur.prior.log <- cand.prior.log
cur.like.log <- cand.like.log
}
output.df[1, ] <- c(mu, sigma)
}
return(output.df)
}
delta005.df <- RunSim(6000, 0.005, log(1.2), 0.5)
delta005.df <- RunSim(6000, 0.005, log(1.2), 0.5)
delta20.df <- RunSim(6000, 0.2, log(1.2), 0.5)
delta80.df <- RunSim(6000, 0.8, log(1.2), 0.5)
install.packages("coda")
install.packages("coda")
library(coda)
traceplot( mcmc( delta20.df[1001:6000, ]))
traceplot(mcmc(delta20.df[1001:6000, ]))
lr <- function(m) exp(m[, 1] + 0.5 * m[, 2] ^ 2)
lrsd <- function(m) sqrt((exp(m[, 2] ^ 2) - 1) * exp(2 * m[, 1] + m[, 2] ^ 2))
plot(100 * lr(delta005.df[1001 : 6000, ]), type = "l", ylim = c(62, 145),
+ ylab = "mean of Loss ratio in (in %)")
plot(100 * lr(delta005.df[1001 : 6000, ]), type = "l", ylim = c(62, 145),
ylab = "mean of Loss ratio in (in %)")
abline(h = 100, col = "grey", lty = 2)
plot (100 * lrsd(delta005.df[1001:6000, ]), type = "l", ylim = c(0, 75),
ylab = "Std. Dev of Loss Ratio (in %)")
lr <- function(m) exp(m[, 1] + 0.5 * m[, 2] ^ 2)
lrsd <- function(m) sqrt((exp(m[, 2] ^ 2) - 1) * exp(2 * m[, 1] + m[, 2] ^ 2))
plot(100 * lr(delta005.df[1001 : 6000, ]), type = "l", ylim = c(62, 145),
ylab = "mean of Loss ratio in (in %)")
abline(h = 100, col = "grey", lty = 2)
plot (100 * lrsd(delta005.df[1001:6000, ]), type = "l", ylim = c(0, 75),
ylab = "Std. Dev of Loss Ratio (in %)")
library(coda)
summary(mcmc(delta20.df[1001:6000, ]))
library(devtools)
devtools::install_github("hadley/plyr")
library(devtools)
update.packages()
library(httr)
install.packages('httr')
install.packages("httr")
install.packages("installr")
library(installr)
updateR()
install.packages('ggplot2')
install.packages('dplyr')
install.packages('readr')
install.packages('scales')
install.packages('lubridate')
install.packages('RColorBrewer')
install.packages('httr')
help(print)
help(str_replace)
help(gather)
??gather
help(sample_n)
??sample_n
help(str_detect)
??str_detect
??tiydyr
??tidyr
??replace_na
theme??
??
??theme_
install.packages("xgboost")
library(xgboost)
library(feather)
### This script will be based off the excellent example found at:
### https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/code
library(caret)
library(plyr)
library(xgboost)
library(ranger)
library(nnet)
library(Metrics)
library(ggplot2)
basepath <- 'C:/Users/evanm_000/Documents/GitHub/house-prices'
setwd(basepath)
train.raw <- read.csv(file.path("Data/train.csv"),stringsAsFactors = FALSE)
test.raw <- read.csv(file.path("Data/test.csv"), stringsAsFactors = FALSE)
CONFIRMED_ATTR <- c("MSSubClass","MSZoning","LotArea","LotShape","LandContour","Neighborhood",
"BldgType","HouseStyle","OverallQual","OverallCond","YearBuilt",
"YearRemodAdd","Exterior1st","Exterior2nd","MasVnrArea","ExterQual",
"Foundation","BsmtQual","BsmtCond","BsmtFinType1","BsmtFinSF1",
"BsmtFinType2","BsmtUnfSF","TotalBsmtSF","HeatingQC","CentralAir",
"X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","FullBath","HalfBath",
"BedroomAbvGr","KitchenAbvGr","KitchenQual","TotRmsAbvGrd","Functional",
"Fireplaces","FireplaceQu","GarageType","GarageYrBlt","GarageFinish",
"GarageCars","GarageArea","GarageQual","GarageCond","PavedDrive","WoodDeckSF",
"OpenPorchSF","Fence")
TENTATIVE_ATTR <- c("Alley","LandSlope","Condition1","RoofStyle","MasVnrType","BsmtExposure",
"Electrical","EnclosedPorch","SaleCondition")
REJECTED_ATTR <- c("LotFrontage","Street","Utilities","LotConfig","Condition2","RoofMatl",
"ExterCond","BsmtFinSF2","Heating","LowQualFinSF","BsmtHalfBath",
"X3SsnPorch","ScreenPorch","PoolArea","PoolQC","MiscFeature","MiscVal",
"MoSold","YrSold","SaleType")
PREDICTOR_ATTR <- c(CONFIRMED_ATTR,TENTATIVE_ATTR,REJECTED_ATTR)
# Determine data types in the data set
data_types <- sapply(PREDICTOR_ATTR,function(x){class(train.raw[[x]])})
unique_data_types <- unique(data_types)
# Separate attributes by data type
DATA_ATTR_TYPES <- lapply(unique_data_types,function(x){ names(data_types[data_types == x])})
names(DATA_ATTR_TYPES) <- unique_data_types
set.seed(13)
data_folds <- createFolds(train.raw$SalePrice, k=5)
# Feature Set 1 - Boruta Confirmed and tentative Attributes
prepL0FeatureSet1 <- function(df) {
id <- df$Id
if (class(df$SalePrice) != "NULL") {
y <- log(df$SalePrice)
} else {
y <- NULL
}
predictor_vars <- c(CONFIRMED_ATTR,TENTATIVE_ATTR)
predictors <- df[predictor_vars]
# for numeric set missing values to -1 for purposes
num_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$integer)
for (x in num_attr){
predictors[[x]][is.na(predictors[[x]])] <- -1
}
# for character  atributes set missing value
char_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$character)
for (x in char_attr){
predictors[[x]][is.na(predictors[[x]])] <- "*MISSING*"
predictors[[x]] <- factor(predictors[[x]])
}
return(list(id=id,y=y,predictors=predictors))
}
L0FeatureSet1 <- list(train=prepL0FeatureSet1(train.raw),
test=prepL0FeatureSet1(test.raw))
# Feature Set 2 (xgboost) - Boruta Confirmed Attributes
prepL0FeatureSet2 <- function(df) {
id <- df$Id
if (class(df$SalePrice) != "NULL") {
y <- log(df$SalePrice)
} else {
y <- NULL
}
predictor_vars <- c(CONFIRMED_ATTR,TENTATIVE_ATTR)
predictors <- df[predictor_vars]
# for numeric set missing values to -1 for purposes
num_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$integer)
for (x in num_attr){
predictors[[x]][is.na(predictors[[x]])] <- -1
}
# for character  atributes set missing value
char_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$character)
for (x in char_attr){
predictors[[x]][is.na(predictors[[x]])] <- "*MISSING*"
predictors[[x]] <- as.numeric(factor(predictors[[x]]))
}
return(list(id=id,y=y,predictors=as.matrix(predictors)))
}
L0FeatureSet2 <- list(train=prepL0FeatureSet2(train.raw),
test=prepL0FeatureSet2(test.raw))
#train model on one data fold
trainOneFold <- function(this_fold,feature_set) {
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
set.seed(825)
fitted_mdl <- do.call(caret::train,
c(list(x=train.data$predictors,y=train.data$y),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- rmse(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
# make prediction from a model fitted to one fold
makeOneFoldTestPrediction <- function(this_fold,feature_set) {
fitted_mdl <- this_fold$fitted_mdl
yhat <- predict(fitted_mdl,newdata = feature_set$test$predictors,type = "raw")
return(yhat)
}
# set caret training parameters
CARET.TRAIN.PARMS <- list(method="gbm")
CARET.TUNE.GRID <-  expand.grid(n.trees=100,
interaction.depth=10,
shrinkage=0.1,
n.minobsinnode=10)
MODEL.SPECIFIC.PARMS <- list(verbose=0) #NULL # Other model specific parameters
# model specific training parameter
CARET.TRAIN.CTRL <- trainControl(method="none",
verboseIter=FALSE,
classProbs=FALSE)
CARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,
tuneGrid=CARET.TUNE.GRID,
metric="RMSE")
# generate features for Level 1
gbm_set <- llply(data_folds,trainOneFold,L0FeatureSet1)
# final model fit
gbm_mdl <- do.call(caret::train,
c(list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
# CV Error Estimate
cv_y <- do.call(c,lapply(gbm_set,function(x){x$predictions$y}))
cv_yhat <- do.call(c,lapply(gbm_set,function(x){x$predictions$yhat}))
rmse(cv_y,cv_yhat)
cat("Average CV rmse:",mean(do.call(c,lapply(gbm_set,function(x){x$score}))))
# create test submission.
# A prediction is made by averaging the predictions made by using the models
# fitted for each fold.
test_gbm_yhat <- predict(gbm_mdl,newdata = L0FeatureSet1$test$predictors,type = "raw")
gbm_submission <- cbind(Id=L0FeatureSet1$test$id,SalePrice=exp(test_gbm_yhat))
write.csv(gbm_submission,file="Model outputs/gbm_sumbission.csv",row.names=FALSE)
gbm_submission <- cbind(Id=L0FeatureSet1$test$id,SalePrice=test_gbm_yhat)
write.csv(gbm_submission,file="Model outputs/gbm_logged.csv",row.names=FALSE)
# set caret training parameters for ranger model
CARET.TRAIN.PARMS <- list(method="ranger")
CARET.TUNE.GRID <-  expand.grid(mtry=2*as.integer(sqrt(ncol(L0FeatureSet1$train$predictors))))
MODEL.SPECIFIC.PARMS <- list(verbose=0,num.trees=500) #NULL # Other model specific parameters
# model specific training parameter
CARET.TRAIN.CTRL <- trainControl(method="none",
verboseIter=FALSE,
classProbs=FALSE)
CARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,
tuneGrid=CARET.TUNE.GRID,
metric="RMSE")
# generate Level 1 features
rngr_set <- llply(data_folds,trainOneFold,L0FeatureSet1)
# final model fit
rngr_mdl <- do.call(caret::train,
c(list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
# CV Error Estimate
cv_y <- do.call(c,lapply(rngr_set,function(x){x$predictions$y}))
cv_yhat <- do.call(c,lapply(rngr_set,function(x){x$predictions$yhat}))
rmse(cv_y,cv_yhat)
cat("Average CV rmse:",mean(do.call(c,lapply(rngr_set,function(x){x$score}))))
# create test submission.
# A prediction is made by averaging the predictions made by using the models
# fitted for each fold.
test_rngr_yhat <- predict(rngr_mdl,newdata = L0FeatureSet1$test$predictors,type = "raw")
rngr_submission <- cbind(Id=L0FeatureSet1$test$id,SalePrice=exp(test_rngr_yhat))
write.csv(rngr_submission,file="Model outputs/rngr_sumbission.csv",row.names=FALSE)
rngr_submission <- cbind(Id=L0FeatureSet1$test$id,SalePrice=test_rngr_yhat)
write.csv(rngr_submission,file="Model outputs/rngr_logged.csv",row.names=FALSE)
## Level 1 Model Training
### Create predictions For Level 1 Model
# gbm_yhat <- do.call(c,lapply(gbm_set,function(x){x$predictions$yhat}))
# rngr_yhat <- do.call(c,lapply(rngr_set,function(x){x$predictions$yhat}))
gbm_yhat <- read.csv('Model outputs/gbm_logged.csv', header = TRUE)$SalePrice
rngr_yhat <- read.csv('Model outputs/rngr_logged.csv', header = TRUE)$SalePrice
xgboost_yhat <- read.csv('Model outputs/xgboost_logged.csv', header = TRUE)$SalePrice
lasso_yhat <- read.csv('Model outputs/lassocv_logged.csv', header = TRUE)$SalePrice
# create Feature Set
L1FeatureSet <- list()
L1FeatureSet$train$id <- do.call(c,lapply(gbm_set,function(x){x$predictions$ID}))
L1FeatureSet$train$y <- do.call(c,lapply(gbm_set,function(x){x$predictions$y}))
predictors <- data.frame(gbm_yhat,rngr_yhat, xgboost_yhat, lasso_yhat)
predictors_rank <- t(apply(predictors,1,rank))
colnames(predictors_rank) <- paste0("rank_",names(predictors))
L1FeatureSet$train$predictors <- predictors #cbind(predictors,predictors_rank)
L1FeatureSet$test$id <- gbm_submission[,"Id"]
L1FeatureSet$test$predictors <- data.frame(gbm_yhat=gbm_yhat,
rngr_yhat=rngr_yhat,
xgboost_yhat = xgboost_yhat,
lasso_yhat = lasso_yhat)
CARET.TRAIN.PARMS <- list(method="nnet")
CARET.TUNE.GRID <-  NULL  # NULL provides model specific default tuning parameters
# model specific training parameter
CARET.TRAIN.CTRL <- trainControl(method="repeatedcv",
number=5,
repeats=1,
verboseIter=FALSE)
CARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,
maximize=FALSE,
tuneGrid=CARET.TUNE.GRID,
tuneLength=7,
metric="RMSE")
MODEL.SPECIFIC.PARMS <- list(verbose=FALSE,linout=TRUE,trace=FALSE) #NULL # Other model specific parameters
# train the model
set.seed(825)
l1_nnet_mdl <- do.call(caret::train,c(list(x=L1FeatureSet$train$predictors,y=L1FeatureSet$train$y),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
length(L1FeatureSet$train$y)
L1FeatureSet$train$y
length(L1FeatureSet$train$y)
l1_nnet_mdl <- do.call(caret::train,c(list(x=L1FeatureSet$train$predictors,y=L1FeatureSet$train$y[1:1459]),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
l1_nnet_mdl <- do.call(caret::train,c(list(x=as.numeric(L1FeatureSet$train$predictors),y=L1FeatureSet$train$y[1:1459]),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
l1_nnet_mdl <- do.call(caret::train,c(list(x=L1FeatureSet$train$predictors,y=L1FeatureSet$train$y[1:1459]),
CARET.TRAIN.PARMS,
MODEL.SPECIFIC.PARMS,
CARET.TRAIN.OTHER.PARMS))
